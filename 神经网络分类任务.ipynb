{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "floating-space",
   "metadata": {},
   "source": [
    "# Mnist分类任务\n",
    "网络基本构建与训练方法\n",
    "torch.nn.functional 模块\n",
    "nn.Module模块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-twist",
   "metadata": {},
   "source": [
    "# 读取Mnist数据集\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ranging-radio",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "corrected-operations",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = requests.session()\n",
    "s.keep_alive = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "designed-regulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "#DATA_PATH = Path(\"data\")\n",
    "#PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "#PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#URL = \"http://deeplearning.net/data/mnist/\"\n",
    "#FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "#if not (PATH / FILENAME).exists():\n",
    "#        content = requests.get(URL + FILENAME).content\n",
    "#        (PATH / FILENAME).open(\"wb\").write(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "blond-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "with gzip.open((PATH/FILENAME).as_posix(),\"rb\") as f:\n",
    "    ((x_train,y_train),(x_valid,y_valid),_) = pickle.load(f, encoding = \"latin-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-apartment",
   "metadata": {},
   "source": [
    "784是mnist数据集每个样本的像素点个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "wrapped-jacket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "\n",
    "pyplot.imshow(x_train[0].reshape((28,28)),cmap = \"gray\")\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "refined-management",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])\n",
      "torch.Size([50000, 784])\n",
      "tensor(0) tensor(9)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x_train, y_train, x_valid, y_valid = map(\n",
    "    torch.tensor,(x_train,y_train,x_valid,y_valid)\n",
    ")\n",
    "n, c = x_train.shape\n",
    "x_train, x_train.shape, y_train.min(), y_train.max()\n",
    "print(x_train, y_train)\n",
    "print(x_train.shape)\n",
    "print(y_train.min(), y_train.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-kitchen",
   "metadata": {},
   "source": [
    "# torch.nn.functional 很多层和函数都会在这里见到\n",
    "torch.nn.functional中含有很多功能，后续会常用。一般情况下，如果模型有可学习的参数，最好用nn.Module 其他情况下nn.functional相对简单一点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "working-experience",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "loss_func = F.cross_entropy\n",
    "def model(xb):\n",
    "    return xb.mm(weights) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "deadly-lancaster",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14.3712, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "bs = 64\n",
    "xb = x_train[0:bs]  # a mini-batch from x\n",
    "yb = y_train[0:bs]\n",
    "weights = torch.randn([784, 10], dtype = torch.float,  requires_grad = True) \n",
    "bs = 64\n",
    "bias = torch.zeros(10, requires_grad=True)\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-professional",
   "metadata": {},
   "source": [
    "# 创建一个model来更简化代码\n",
    "必须继承nn.Module 且在其构造函数中需调用nn.Module的构造函数$\\\\\n",
    "无需写反向传播函数，nn.Module能够利用autograd自动实现反向传播$\\\\\n",
    "Module中的课学习参数可以通过named_parameters()或者 parameters()返回迭代器$\\\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "patient-terry",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Mnist_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(784, 128)\n",
    "        self.hidden2 = nn.Linear(128,256)\n",
    "        self.out = nn.Linear(256,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "delayed-validity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mnist_NN(\n",
      "  (hidden1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (hidden2): Linear(in_features=128, out_features=256, bias=True)\n",
      "  (out): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Mnist_NN()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marked-rotation",
   "metadata": {},
   "source": [
    "可以打印我们定义好名字里的权重和偏置量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "clinical-patch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden1.weight Parameter containing:\n",
      "tensor([[ 0.0341,  0.0110,  0.0148,  ...,  0.0193, -0.0234, -0.0005],\n",
      "        [ 0.0054,  0.0252,  0.0202,  ...,  0.0285, -0.0047,  0.0347],\n",
      "        [-0.0271,  0.0278, -0.0336,  ..., -0.0070, -0.0100, -0.0325],\n",
      "        ...,\n",
      "        [-0.0347, -0.0184, -0.0204,  ...,  0.0011, -0.0068,  0.0356],\n",
      "        [-0.0304, -0.0282, -0.0073,  ...,  0.0304,  0.0305, -0.0172],\n",
      "        [-0.0062, -0.0042,  0.0280,  ...,  0.0060,  0.0170,  0.0216]],\n",
      "       requires_grad=True) torch.Size([128, 784])\n",
      "hidden1.bias Parameter containing:\n",
      "tensor([ 1.1069e-02, -4.6443e-03,  8.0656e-03,  1.7993e-02,  2.9170e-02,\n",
      "         5.8220e-03, -7.4386e-03, -1.4182e-02, -3.2990e-02,  2.6046e-02,\n",
      "         2.8509e-02, -2.0925e-02,  1.8690e-02,  2.9148e-02, -2.4941e-02,\n",
      "         2.5654e-02,  1.6916e-02,  3.2241e-02,  1.5787e-02, -3.0654e-02,\n",
      "         2.9504e-02,  2.7130e-02,  1.0985e-02,  2.7189e-02,  1.8916e-02,\n",
      "        -9.4005e-03,  1.9020e-02, -1.5853e-02,  3.4862e-02, -3.2194e-02,\n",
      "        -2.3730e-02, -2.4465e-02,  1.4179e-02,  2.7098e-03, -1.4949e-02,\n",
      "         1.4216e-02,  1.6566e-02, -3.0176e-02,  9.1345e-03, -3.2573e-02,\n",
      "        -3.1060e-02, -1.6765e-02, -1.3928e-02, -3.3113e-02, -1.3454e-02,\n",
      "         2.2221e-02, -1.7461e-02, -2.0047e-02, -2.9834e-02,  1.4236e-02,\n",
      "         1.3574e-02, -2.7450e-02,  1.3893e-03, -3.5331e-02, -2.6708e-02,\n",
      "        -4.7722e-04, -4.1904e-03, -2.3660e-02, -3.0952e-02,  8.1275e-03,\n",
      "         1.4068e-02, -2.3880e-02, -1.7951e-02, -8.5436e-04, -2.9155e-02,\n",
      "        -2.5606e-02, -1.4453e-02,  2.3389e-02, -2.4628e-02, -3.2480e-02,\n",
      "         9.2511e-03, -3.1471e-02, -2.5673e-02, -8.0202e-03, -3.1280e-02,\n",
      "         4.0692e-03,  2.0224e-02, -1.6863e-02,  1.3968e-03,  9.8750e-03,\n",
      "         1.5239e-02,  1.8396e-02,  3.3822e-02,  2.3141e-02,  2.4359e-02,\n",
      "        -1.9794e-02,  2.3135e-02,  1.9600e-02,  5.3970e-03,  1.8943e-02,\n",
      "         9.7800e-03,  3.1060e-02, -3.1919e-03, -2.5179e-02, -2.8229e-02,\n",
      "         6.4230e-03, -1.1610e-02,  2.8609e-02,  2.3121e-02, -1.3127e-02,\n",
      "         1.7383e-02,  1.9713e-02, -3.5197e-02, -1.5988e-02,  2.6008e-02,\n",
      "         2.1499e-02, -1.8053e-02, -7.5184e-05, -7.7925e-03,  1.7893e-02,\n",
      "        -2.5236e-02,  3.3243e-02, -3.0141e-02,  3.3047e-03, -2.9034e-02,\n",
      "         3.5688e-02,  9.5397e-03, -1.8704e-02, -5.6176e-04, -1.8126e-02,\n",
      "         2.2617e-02, -2.6303e-02,  3.1166e-02,  2.5573e-02, -1.6063e-02,\n",
      "        -3.1239e-02,  2.8879e-02,  1.4703e-02], requires_grad=True) torch.Size([128])\n",
      "hidden2.weight Parameter containing:\n",
      "tensor([[-0.0683, -0.0554,  0.0550,  ..., -0.0324, -0.0033,  0.0584],\n",
      "        [ 0.0204,  0.0474, -0.0524,  ..., -0.0481, -0.0365,  0.0461],\n",
      "        [-0.0031,  0.0644, -0.0138,  ...,  0.0443, -0.0630,  0.0195],\n",
      "        ...,\n",
      "        [ 0.0305,  0.0073, -0.0520,  ..., -0.0631, -0.0230, -0.0817],\n",
      "        [ 0.0591, -0.0224,  0.0353,  ...,  0.0160, -0.0870, -0.0420],\n",
      "        [ 0.0793, -0.0420,  0.0169,  ...,  0.0403, -0.0626, -0.0163]],\n",
      "       requires_grad=True) torch.Size([256, 128])\n",
      "hidden2.bias Parameter containing:\n",
      "tensor([-8.4789e-02,  6.9248e-02, -1.3071e-02, -7.3342e-02,  3.3946e-02,\n",
      "        -1.0570e-02,  6.1307e-03,  1.1247e-02,  4.9050e-02,  5.7570e-02,\n",
      "         4.3887e-02,  8.3927e-02,  8.1880e-02, -5.8145e-02, -6.6141e-02,\n",
      "         4.5452e-02,  3.9415e-02, -3.1212e-02,  6.1856e-03,  6.1885e-02,\n",
      "         1.5397e-02,  6.4557e-02, -6.9247e-02,  4.6045e-02, -5.2076e-02,\n",
      "        -8.5300e-02, -8.3708e-02, -3.1576e-02,  5.2096e-02,  3.7338e-02,\n",
      "        -8.3365e-02,  4.9182e-02, -1.7678e-02, -2.2172e-02, -8.1402e-02,\n",
      "        -5.3840e-02,  6.4068e-02,  8.6391e-02, -5.7807e-02,  5.8098e-02,\n",
      "        -5.3848e-03,  3.6926e-02,  4.5095e-02,  3.6184e-02,  6.8218e-05,\n",
      "         1.3941e-02, -5.1679e-02, -6.1473e-02, -5.8585e-02,  2.3604e-02,\n",
      "        -1.4700e-02,  6.9610e-02,  2.0570e-03,  5.3329e-02, -6.2079e-04,\n",
      "        -3.7583e-02,  7.3529e-02,  4.4623e-02, -7.5062e-02,  4.8834e-02,\n",
      "        -7.2726e-02,  3.1855e-02,  3.1812e-02,  7.2653e-02, -2.3479e-02,\n",
      "         4.8050e-02,  2.4289e-02, -1.6929e-02, -2.2771e-03,  4.3112e-03,\n",
      "        -8.4492e-02, -7.6936e-02, -4.1211e-02,  7.8355e-03,  5.8958e-02,\n",
      "         6.1859e-02, -3.3915e-02, -7.9065e-02, -6.1631e-02,  1.8020e-02,\n",
      "        -8.1377e-02,  3.3563e-02,  4.9079e-02, -8.2888e-02, -5.4439e-02,\n",
      "         2.8989e-03, -7.9156e-03, -2.4468e-02,  6.5063e-02,  5.1444e-02,\n",
      "         5.8484e-02,  5.0865e-02,  3.3882e-02,  5.2962e-02,  4.1857e-02,\n",
      "         8.4950e-03,  6.4998e-03, -2.6457e-02,  5.0572e-02,  5.5605e-02,\n",
      "        -6.4285e-02, -5.5939e-02, -2.2977e-02,  2.0319e-02, -7.4809e-02,\n",
      "        -7.9342e-02,  5.3484e-02,  6.0160e-02,  1.5000e-02, -2.4874e-03,\n",
      "         4.1871e-02,  3.8429e-02,  1.7100e-03, -1.0760e-02,  5.6052e-03,\n",
      "         6.0247e-03, -5.1900e-02, -2.4871e-02, -3.5958e-02, -5.2547e-02,\n",
      "        -3.1697e-02, -1.6913e-02, -1.0797e-02,  4.4688e-02, -1.7820e-02,\n",
      "         2.0681e-03,  6.6113e-03, -8.1107e-02, -5.3037e-02, -7.4887e-02,\n",
      "        -6.5950e-02, -1.6216e-02, -8.4036e-02,  3.5728e-02, -3.3263e-02,\n",
      "        -5.3556e-02,  3.7082e-02, -3.9663e-02, -1.6042e-02, -4.4092e-03,\n",
      "         4.0048e-02,  4.9050e-02, -8.3690e-02,  5.4646e-02,  1.6735e-02,\n",
      "         5.9533e-02, -3.7074e-02,  3.7238e-02,  4.3106e-02, -1.7257e-02,\n",
      "        -4.0745e-02,  5.5779e-02,  4.6798e-02,  6.7471e-02, -6.4478e-02,\n",
      "         1.7104e-02,  7.6548e-02, -4.7154e-02, -8.4055e-02,  2.5528e-02,\n",
      "        -6.6627e-03, -6.6953e-02, -4.2821e-02,  4.5191e-02,  1.4513e-02,\n",
      "        -7.5948e-02,  1.2099e-02, -3.8351e-03,  4.2260e-02, -2.5487e-03,\n",
      "        -8.0848e-02,  1.2173e-02,  7.9188e-02, -7.8742e-02,  3.1519e-02,\n",
      "        -5.3772e-03,  1.9797e-02,  3.9906e-02,  1.6615e-02, -7.5745e-02,\n",
      "         7.1765e-02, -8.0929e-02, -3.7326e-02, -4.7066e-03, -8.1904e-02,\n",
      "        -7.5120e-02, -5.4700e-02,  3.3435e-02, -2.4049e-02, -4.4691e-02,\n",
      "        -6.0955e-02, -7.2140e-02, -3.4897e-02, -5.8519e-03,  2.8251e-02,\n",
      "        -6.7888e-02,  3.0482e-02, -2.7362e-03,  6.6316e-02,  3.5772e-02,\n",
      "         2.1072e-02, -1.5054e-02,  7.5845e-02,  1.9026e-02,  7.7322e-02,\n",
      "        -7.0479e-02,  7.2418e-02,  3.4646e-03, -1.5308e-02, -1.2283e-02,\n",
      "         4.6717e-02, -8.5898e-02, -2.3468e-02, -4.5926e-02, -2.2754e-02,\n",
      "         4.5509e-02, -5.0535e-02, -6.6403e-02,  4.4664e-02, -6.7252e-02,\n",
      "         4.3424e-02,  5.6104e-02, -7.4039e-03, -4.2265e-02,  3.3580e-02,\n",
      "         1.5679e-02,  5.8465e-02, -6.5353e-02,  3.6980e-02, -8.2903e-05,\n",
      "         6.7123e-02, -1.4469e-02,  8.7896e-02, -6.4797e-02,  7.8923e-02,\n",
      "        -4.6016e-02,  4.2420e-02,  8.5882e-02,  8.2330e-02,  5.9916e-02,\n",
      "        -8.4736e-02, -6.5265e-02,  5.8605e-02,  8.5324e-02, -1.4921e-02,\n",
      "        -8.3146e-02, -4.0310e-02,  6.8967e-02, -5.3135e-02, -4.6953e-02,\n",
      "        -6.2501e-02,  6.3677e-02, -1.7431e-02, -6.4091e-02,  6.5169e-03,\n",
      "        -4.0502e-02], requires_grad=True) torch.Size([256])\n",
      "out.weight Parameter containing:\n",
      "tensor([[-0.0207,  0.0324,  0.0479,  ...,  0.0103, -0.0150, -0.0573],\n",
      "        [-0.0420, -0.0081,  0.0002,  ..., -0.0025, -0.0043, -0.0544],\n",
      "        [ 0.0186,  0.0227, -0.0026,  ...,  0.0241, -0.0521, -0.0172],\n",
      "        ...,\n",
      "        [ 0.0619, -0.0556, -0.0500,  ..., -0.0180, -0.0396,  0.0277],\n",
      "        [-0.0214, -0.0111,  0.0512,  ...,  0.0455, -0.0564,  0.0148],\n",
      "        [-0.0132, -0.0511,  0.0295,  ..., -0.0175,  0.0106,  0.0305]],\n",
      "       requires_grad=True) torch.Size([10, 256])\n",
      "out.bias Parameter containing:\n",
      "tensor([-0.0552, -0.0081, -0.0222,  0.0038,  0.0462, -0.0018, -0.0009, -0.0118,\n",
      "         0.0463,  0.0195], requires_grad=True) torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name,parameter in net.named_parameters():\n",
    "    print(name, parameter, parameter.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-bibliography",
   "metadata": {},
   "source": [
    "# 使用TensorDataset 和 DataLoader来简化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "parallel-lewis",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size = bs, shuffle = True)\n",
    "\n",
    "valid_ds = TensorDataset(x_valid, y_valid)\n",
    "valid_dl = DataLoader(valid_ds, batch_size = bs* 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "unavailable-chance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_ds, valid_ds, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size= bs, shuffle=True),\n",
    "        DataLoader(valid_ds,batch_size = bs *2),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "owned-garlic",
   "metadata": {},
   "source": [
    "一般在训练模型时加上model.train(),这样会正常使用Batch Normalization 和Dropout\n",
    "测试的时候一般选择model.eval(),这样就不会使用Batch Normalization 和 Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "simplified-roberts",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fit(steps, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for step in range(steps):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            loss_batch(model, loss_func, xb, yb, opt)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(\n",
    "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
    "            )\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) /np.sum(nums)\n",
    "        print('当前step:'+str(step),'验证集损失：'+str(val_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "senior-pizza",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "def get_model():\n",
    "    model = Mnist_NN()\n",
    "    return model, optim.SGD(model.parameters(), lr = 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "polyphonic-township",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, opt = None):\n",
    "    loss = loss_func(model(xb),yb)\n",
    "    \n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    return loss.item(), len(xb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "interior-switzerland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前step:0 验证集损失：2.278326903152466\n",
      "当前step:1 验证集损失：2.244901566696167\n",
      "当前step:2 验证集损失：2.1941456745147705\n",
      "当前step:3 验证集损失：2.111106593322754\n",
      "当前step:4 验证集损失：1.9778085941314698\n",
      "当前step:5 验证集损失：1.780006621170044\n",
      "当前step:6 验证集损失：1.5291592222213746\n",
      "当前step:7 验证集损失：1.2778354616165162\n",
      "当前step:8 验证集损失：1.0736648317337036\n",
      "当前step:9 验证集损失：0.9232409603118896\n",
      "当前step:10 验证集损失：0.813596106338501\n",
      "当前step:11 验证集损失：0.731258726978302\n",
      "当前step:12 验证集损失：0.667184289264679\n",
      "当前step:13 验证集损失：0.6167958295822143\n",
      "当前step:14 验证集损失：0.5752523943901062\n",
      "当前step:15 验证集损失：0.541217329454422\n",
      "当前step:16 验证集损失：0.512826971244812\n",
      "当前step:17 验证集损失：0.48907250604629515\n",
      "当前step:18 验证集损失：0.4687370072364807\n",
      "当前step:19 验证集损失：0.45105227928161623\n",
      "当前step:20 验证集损失：0.436068651342392\n",
      "当前step:21 验证集损失：0.42317190113067626\n",
      "当前step:22 验证集损失：0.4112448613643646\n",
      "当前step:23 验证集损失：0.4007654628753662\n",
      "当前step:24 验证集损失：0.39168376955986023\n"
     ]
    }
   ],
   "source": [
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "model, opt =get_model()\n",
    "fit(25,model,loss_func, opt,train_dl, valid_dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-pioneer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
